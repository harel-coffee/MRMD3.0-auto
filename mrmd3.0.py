# !/usr/bin/env python3
# -*- coding=utf-8 -*-
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
from sklearn.preprocessing import label_binarize
import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
import argparse
import sklearn.metrics
import time
import logging
import os,math
from scipy.io import arff
from sklearn.datasets import load_svmlight_file
from sklearn.datasets import dump_svmlight_file
from format import pandas2arff
from sklearn.manifold import TSNE
from math import ceil
from sklearn.preprocessing import LabelBinarizer,MinMaxScaler
from feature_rank import feature_rank
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import time



def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--start", dest='s', type=int, help="start index", default=1)
    parser.add_argument("-i", "--inputfile", dest='i', type=str, help="input file", required=True)
    parser.add_argument("-e", "--end", dest='e', type=int, help="end index", default=-1)
    parser.add_argument("-l", "--length", dest='l', type=int, help="step length", default=1)
    parser.add_argument("-n", "--n_dim", dest='n', type=int, help="save feature top n", default=-1)
    parser.add_argument("-t", "--type_metric", dest='t', type=str, help="evaluation metric(f1, acc, recall,precision, auc)", default="f1")
    # parser.add_argument("-m", "--metrics_file", dest='m', type=str, help="output the metrics file", default=None)
    parser.add_argument("-c", "--classifier", dest='c', type=str, help="classifier(RandomForest,SVM,Bayes)",
                        default="RandomForest", choices=["RandomForest", "SVM", "Bayes"])
    parser.add_argument("-o", "--outfile", dest='o', type=str, help="output the dimensionality reduction file")
    # parser.add_argument("-p", "--picture", dest='p', type=str, default='false',
    #                     help="The scatter plots before and after dimension reduction are generated by tsne")
    parser.add_argument("-f","--topn", dest='f', type=int, default=15,
                        help="select top n features to chart")
    parser.add_argument("-g","--config", dest='g', type=str, default="False",
                        help="the config file for select feature rank methods")
    parser.add_argument("-r", "--rank_method", dest='r', type=str, help="the rank method for features",
                        choices=["PageRank",
                                 "Hits_a",
                                 "Hits_h",
                                 "LeaderRank",
                                 "TrustRank"],
                        default="PageRank")
    parser.add_argument("--seed", dest='seed', type=str, help="fix results")
    parser.add_argument("--light", dest='light', type=str,default="false", help="use fast version(delete some features rank methods with high time complexity)")
    from sklearn.preprocessing import label_binarize
    args = parser.parse_args()
    return args


class Dim_Rd(object):
    def __init__(self, file_csv, logger):
        self.file_csv = file_csv
        self.logger = logger


    def read_data(self):  # default csv

        def read_csv():
            self.df = pd.read_csv(self.file_csv, engine='python').dropna(axis=1)
            datas = np.array(self.df)
            self.datas = datas
            self.X = datas[:, 1:]
            self.y = datas[:, 0]

        file_type = self.file_csv.split('.')[-1]
        if file_type == 'csv':
            read_csv()

    def range_steplen(self, start=1, end=1, length=1):
        self.start = start
        self.end = end
        self.length = length

    def classifier(self, X, y):

        if args.c == "SVM":
            clf = LinearSVC( probability=True)
        elif args.c == "Bayes":
            clf = GaussianNB()
        else:
            clf = RandomForestClassifier()


        proba = sklearn.model_selection.cross_val_predict(clf, X, y, method="predict_proba",n_jobs=-1, cv=5)
        ypred = np.argmax(proba, axis=1)

        f1 = sklearn.metrics.f1_score(y, ypred, average='weighted')
        precision = sklearn.metrics.precision_score(self.y, ypred, average='weighted')
        recall = sklearn.metrics.recall_score(self.y, ypred, average='weighted')
        acc = sklearn.metrics.accuracy_score(self.y, ypred)
        if len(set(y)) ==2:
            auc = sklearn.metrics.roc_auc_score(y, proba[:,1])
        else:
            n_classes = len(np.unique(y))
            # 使用label_binarize将y_true转换为二进制矩阵
            y_true_bin = label_binarize(y, classes=np.arange(n_classes))
            # 计算多分类任务的AUC
            auc = sklearn.metrics.roc_auc_score(y_true_bin, proba, multi_class='ovr')
        return acc, f1, precision, recall, auc, ypred

    def Result(self, seqmax, clf, features, csvfile):
        #ypred = sklearn.model_selection.cross_val_predict(clf, self.X[:, seqmax], self.y, n_jobs=-1, cv=5)
        proba = sklearn.model_selection.cross_val_predict(clf, self.X[:, seqmax], self.y, method="predict_proba",n_jobs=-1, cv=5)
        ypred = np.argmax(proba, axis=1)
        cm = pd.crosstab(pd.Series(self.y, name='Actual'), pd.Series(ypred, name='Predicted'))
        confusion_matrix = str('***confusion matrix***' + os.linesep + str(cm))
        logger.info(confusion_matrix)
        f1 = sklearn.metrics.f1_score(self.y, ypred, average='weighted')
        # f.write('f1 ={}\n '.format(f1))
        logger.info(('f1 ={:0.4f} '.format(f1)))
        acc = sklearn.metrics.accuracy_score(self.y, ypred)
        logger.info('accuarcy = {:0.4f} '.format(acc))
        # f.write('accuarcy = {:} \n'.format(acc))
        precision = sklearn.metrics.precision_score(self.y, ypred, average='weighted')
        logger.info('precision ={:0.4f} '.format(precision))
        # f.write('precision ={} \n'.format(precision))
        recall = sklearn.metrics.recall_score(self.y, ypred, average='weighted')
        logger.info(('recall ={:0.4f}'.format(recall)))
        # f.write('recall ={}\n '.format(recall))
        # lb = LabelBinarizer()
        # y = lb.fit_transform(self.y)
        # ypred = lb.transform(ypred)
        if len(set(self.y)) >2:
            y = self.y
            n_classes = len(np.unique(y))
            # 使用label_binarize将y_true转换为二进制矩阵
            y_true_bin = label_binarize(y, classes=np.arange(n_classes))
            # 计算多分类任务的AUC
            auc = sklearn.metrics.roc_auc_score(y_true_bin, proba, multi_class='ovr')
        else:
            auc = sklearn.metrics.roc_auc_score(self.y,proba[:, 1])
        # f.write('roc area = {}\n'.format(roc))
        logger.info('auc = {:0.4f}'.format(auc))

        columns_index = [0]
        columns_index.extend([i + 1 for i in seqmax])
        data = np.concatenate((self.y.reshape(len(self.y), 1), self.X[:, seqmax]), axis=1)
        features_list = (self.df.columns.values)

        if args.n == -1:
            pass
        else:
            columns_index = columns_index[0:args.n + 1]
            data = data[:, 0:args.n + 1]
        df = pd.DataFrame(data, columns=features_list[columns_index])
        df.iloc[0, :].astype(int)
        df.to_csv(csvfile, index=None)

    def Dim_reduction(self, features, features_sorted, outfile, csvfile):
        logger.info("Start dimension reduction ...")
        features_number = []
        for value in features_sorted:
            features_number.append(features[value[0]] - 1)
        stepSum = 0
        max = 0
        seqmax = []
        predmax = []
        scorecsv = outfile

        with open(scorecsv, 'w') as f:
            f.write('length,accuracy,f1,precision,recall,auc\n')
            for i in range(int(ceil((self.end - self.start) / self.length)) + 1):
                if (stepSum + self.start) < self.end:
                    n = stepSum + self.start
                else:
                    n = self.end

                stepSum += self.length

                ix = features_number[self.start - 1:n]
                acc, f1, precision, recall, auc, ypred = self.classifier(self.X[:, ix], self.y)

                if args.t == "f1":
                    benchmark = f1
                elif args.t == "acc":
                    benchmark = acc
                elif args.t == "precision":
                    benchmark = precision
                elif args.t == "recall":
                    benchmark = recall
                elif args.t == "auc":
                    benchmark = auc

                if benchmark > max:
                    max = benchmark
                    seqmax = ix

                logger.info(
                    'length={} f1={:0.4f} accuarcy={:0.4f} precision={:0.4f} recall={:0.4f} auc={:0.4f} '.format(
                        len(ix), f1, acc, precision, recall, auc))
                f.write('{},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f}\n'.format(len(ix), acc, f1, precision, recall, auc))

        logger.info('----------')
        logger.info('the max {} = {:0.4f}'.format(args.t, max))

        index_add1 = [x + 1 for x in seqmax]
        logger.info('IFS: {},recommend length = {}'.format(self.df.columns.values[index_add1], len(seqmax)))
        logger.info('-----------')
        if args.c == "SVM":
            clf = LinearSVC(random_state=1, tol=1e-5)
        elif args.c == "Bayes":
            clf = GaussianNB()
        else:
            clf = RandomForestClassifier(random_state=1, n_estimators=100, n_jobs=-1)
        self.Result(seqmax, clf, features, csvfile)
        logger.info('-----------')

        ###
    def run(self, inputfile):

        args = parse_args()
        file = inputfile
        # random seed
        # seed = 1
        # np.random.seed(seed)
        if args.seed:
            print(args.seed)
            seed = int(args.seed)
            np.random.seed(seed)
        metrics_file = ''.join(os.path.basename(args.i).split('.')[:-1]) + '.metrics.csv'
        csvfile = args.o


        features, features_sorted = feature_rank(file, self.logger,args.r,args.g,args.light)

        self.read_data()
        if int(args.e) == -1:
            args.e = len(pd.read_csv(file, engine='python').columns) - 1
        self.range_steplen(args.s, args.e, args.l)
        metrics_file = os.getcwd() + os.sep + 'Results' + os.sep + metrics_file
        csvfile = os.getcwd() + os.sep + 'Results' + os.sep + csvfile
        logger.info("rank result:")
        logger.info([x[0] for x in features_sorted])
        self.Dim_reduction(features, features_sorted, metrics_file, csvfile)
        return features_sorted


def arff2csv(file):

    data = arff.loadarff(file)
    df = pd.DataFrame(data[0])
    df.iloc[:,-1] = df.iloc[:,-1].map(lambda x: x.decode())

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    file_csv = file + '.csv'
    df.to_csv(file_csv, index=None)
    return file_csv

def arff2csv_notfile(file):
    with open(file,"r+") as f:
        conent = []
        for line in  f.readlines():
            if line:
                if "{" in line:
                    line = line.replace("{"," {")
                    conent.append(line)
                else:
                    conent.append(line)
    with open(file,"w+") as f:
        for line in  conent:
            f.write(line)

    data = arff.loadarff(file)
    df = pd.DataFrame(data[0])
    df.iloc[:,0]= df.iloc[:,0].map(lambda x: x.decode())

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    return df

def libsvm2csv_notfile(file):
    data = load_svmlight_file(file)
    df = pd.DataFrame(data[0].todense())
    df.iloc[:,0] = pd.Series(np.array(data[1])).astype(int)
    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    return df

def libsvm2csv(file):
    data = load_svmlight_file(file)
    df = pd.DataFrame(data[0].todense())
    df.iloc[:,0] = pd.Series(np.array(data[1])).astype(int)

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    file_csv = file + '.csv'
    df.to_csv(file_csv, index=None)

    return file_csv


def tsne_scatter(file):

    df = pd.read_csv(file, engine='python')
    tsne = TSNE()

    label_name = df.columns.values[0]
    fea_data = df.drop(columns=[label_name])
    redu_fea = tsne.fit_transform(fea_data)
    labels = df.iloc[:, 0]
    redu_data = np.vstack((redu_fea.T, labels.T)).T
    tsne_df = pd.DataFrame(
        data=redu_data, columns=['Dimension1', 'Dimension2', "label"])

    scaler = MinMaxScaler()
    tsne_df[['Dimension1', 'Dimension2']] = scaler.fit_transform(tsne_df[['Dimension1', 'Dimension2']])
    p1 = tsne_df[(tsne_df.iloc[:, 2] == 1)]
    p2 = tsne_df[(tsne_df.iloc[:, 2] == 0)]

    # 绘制散点图
    # plt.plot(x1, y1, 'o', color="#3dbde2", label='positive', markersize='1')
    # plt.plot(x2, y2, 'o', color="#b41f87", label='negative', markersize='1')
    colorlist = ["#3dbde2","#b41f87",'#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf',
                 "#fdbde2","#341f87",'#541a1c', '#a77eb8', '#edaf4a', '#b84ea3', '#af7f00', '#1fff33', '#765628']

    for label,color in zip(list(set(tsne_df.iloc[:, 2])),colorlist):
        p1 = tsne_df[(tsne_df.iloc[:, 2] == label)]
        x1 = p1.values[:, 0]
        y1 = p1.values[:, 1]
        plt.plot(x1, y1, 'o', color=color, label=str(label), markersize='1')

    plt.xlabel('Dimension1', fontsize=9)
    plt.ylabel('Dimension2', fontsize=9)

    plt.legend(loc="upper right", fontsize="x-small")


if __name__ == '__main__':

    args = parse_args()

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    log_path = os.getcwd() + os.sep + 'Logs' + os.sep
    rq = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))
    log_name = log_path + os.path.basename(args.i)+ rq + '.log'
    logfile = log_name
    fh = logging.FileHandler(logfile, mode='w')
    fh.setLevel(logging.INFO)

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    # logging.basicConfig(level=logging.INFO,
    #                     format='[%(asctime)s]: %(message)s')  # logging.basicConfig函数对日志的输出格式及方式做相关配置
    formatter = logging.Formatter('[%(asctime)s]: %(message)s')
    # 文件
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    # 控制台
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    logger.info("---mrmd 3.0 start----")


    file = args.i
    file_type = file.split('.')[-1]
    if file_type == 'csv':
        pass
    elif file_type == 'arff':
        file = arff2csv(file)
    elif file_type == 'libsvm':
        file = libsvm2csv(file)
    else:
        assert "format error"
    # format : arff or libsvm to csv
    #if args.p != 'false':
    if True:
        plt.figure(figsize=(2 * 4.7, 1 * 4.7))
        plt.subplot(1, 2, 1)
        tsne_scatter(file)

    if int(args.e) == -1:
        args.e = len(pd.read_csv(file, engine='python').columns) - 1

    metricfile = ''.join(os.path.basename(args.i).split('.')[:-1]) + '.metrics.csv'

    df = pd.read_csv(file, engine='python').dropna(axis=1)



    d = Dim_Rd(file, logger)
    features_sorted = d.run(file)

    metrics_file = os.getcwd() + os.sep + 'Results' + os.sep + metricfile
    csvfile = os.getcwd() + os.sep + 'Results' + os.sep + args.o

    feature_rank_file = os.getcwd() + os.sep + 'Results' + os.sep+''.join(os.path.basename(args.i).split('.')[:-1]) + '.feature_rank.csv'
    with open(feature_rank_file,"w+") as f:
        for x in features_sorted:
            content = str(x[0])+"  "+str(x[1])+"\n"
            f.write(content)

    logger.info("The output by the terminal's log has been saved in the {}.".format(logfile))
    logger.info('metrics have been saved in the {}.'.format(metrics_file))

    #if args.p != 'false':
    if True:
        plt.subplot(1, 2, 2)
        tsne_scatter(csvfile)
       # plt.title("Comparison before and after dimensionality reduction (TSNE)")
        pngpath = os.path.abspath('./Results') + os.sep + os.path.splitext(os.path.basename(args.i))[0] + '.tsne.png'
        plt.savefig(pngpath)
        logger.info('Scatter charts visualized by t-SNE dataset has been saved in the {}.'.format(pngpath))

    # 处理输出文件的类型
    outputfile_file_type = args.o.split('.')[-1]
    if outputfile_file_type == 'csv':
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(csvfile))

    elif outputfile_file_type == 'arff':
        df = pd.read_csv(csvfile, engine='python')
        filename, ext = os.path.splitext(args.i)

        label_name = df.columns.values[0]
        if df[label_name].dtype == np.float:
            df[label_name] = df[label_name].astype(int)
        temp = df.iloc[:,0]
        df = df.drop(columns=[label_name], axis=1)
        df[label_name] = temp
        DimensionReduction_filename = os.path.abspath('./Results') + os.sep + args.o
        pandas2arff.pandas2arff(df, filename=r'./Results/{}'.format(args.o), wekaname=filename, cleanstringdata=False,
                                cleannan=True)

        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(DimensionReduction_filename))
        # clean_csv(csvfile)

    elif outputfile_file_type == 'libsvm':
        df = pd.read_csv(csvfile, engine='python')
        label_name = df.columns.values[0]
        for x in df.columns:
            if x.lower() == label_name:
                label = x
                break

        y = df[label]
        X = df.drop(columns=label, axis=1)

        inputfile = args.i
        # filename ,ext = os.path.splitext(inputfile)
        DimensionReduction_filename = os.path.abspath('./Results') + os.sep + args.o
        dump_svmlight_file(X, y, DimensionReduction_filename, zero_based=True, multilabel=False)
        # clean_tmpfile.clean_csv(csvfile)
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(DimensionReduction_filename))
    else:
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(csvfile))
    ### 特征趋势图
    def lineplot_(in_file):
        plt.clf()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:

            df = pd.read_csv(in_file).iloc[:,1:]
        x = [x for x in range(len(df.index.values))]
        new_list = range(math.floor(min(x)), math.ceil(max(x)) + 1)
        if args.t == "auc":
            plt.plot(x,df["auc"])
        elif args.t == "accuracy" or args.t =="acc":
            plt.plot(x, df["accuracy"])
        elif args.t == "precision":
            plt.plot(x, df["precision"])
        elif args.t == "recall":
            plt.plot(x, df["recall"])
        else:
            plt.plot(x, df["f1"])

        plt.title(f"The metrics change according to the feature ranking result ({args.t})",fontsize=18)
        plt.xlabel('features num', fontsize=16)
        plt.ylabel(f'score', fontsize=16)

        plt.savefig(f'./Results/{ os.path.splitext(os.path.basename(csvfile))[0]}.line.png', dpi=300)

    lineplot_(metrics_file)
    #####小提琴图
    def violin(in_file,topn = 15):
        plt.clf()
        plt.subplot(1,1,1)
        scaler = StandardScaler()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:
            df = pd.read_csv(in_file)
        if (len(df.columns))-1 <= topn:
            topn = len(df.columns)-1
        df = df.iloc[:,:topn+1]
        sns.set_theme(color_codes=True)

        # if length<=topn:
        #     length=topn

        label = df.pop(df.columns[0])
        label_list = []

        length = len(df.columns)
        for x in range(length):
            label_list.append(label)

        df.columns.values[:] = ["top" + str(x) for x in range(1, len(df.columns)+1)]

        df[df.columns] = scaler.fit_transform(df[df.columns].to_numpy())
        df = df.melt(var_name='features', value_name='vals')

        df["label"] = pd.concat(label_list, ignore_index=True)
        snsfig = sns.violinplot(x="features", y="vals", data=df, hue=df.columns[-1], palette="muted")
        plt.title(f"top {args.f} features")
        figure = snsfig.get_figure()
        figure.savefig(f'./Results/{ os.path.splitext(os.path.basename(csvfile))[0]}.violin.png', dpi=300)

    violin(csvfile,args.f)

    ####热图
    def heatmap_(in_file, topn = 15):
        plt.clf()
        plt.figure(figsize=(6,5))


        scaler = StandardScaler()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:
            df = pd.read_csv(in_file)
        if topn>= len(df.columns)-1:
            topn = len(df.columns)-1
        df.columns.values[1:]= ["top"+ str(x)for x in range(1,len(df.columns))]
        df = df.iloc[:,:topn+1]  # + label
        sns.set_theme(color_codes=True)
        df[df.columns[1:]] = scaler.fit_transform(df[df.columns[1:]].to_numpy())
        # method = ['pearson', 'kendall', 'spearman']
        df_corr = df.corr()

        sns_fig = sns.heatmap(df_corr, cmap="mako")
        figure = sns_fig.get_figure()
        plt.title(f"top {args.f} features")
        figure.savefig(
            f'./Results/{os.path.splitext(os.path.basename(csvfile))[0]}.heatmap.png',
            dpi=300)

        #plt.show()

    heatmap_(csvfile,args.f)

    ###stem
    def feature_importance_stem_(topn = 15):
        plt.clf()
        xlist = []
        ylist = []
        if topn>= len(features_sorted):
            topn = len(features_sorted)
        xlist = ["top" + str(x) for x in range(1, topn+1)]

        for x, y in features_sorted[:topn]:

            ylist.append(y)

        plt.xlabel(f'feature score({args.r})', fontsize=18)
        plt.ylabel('features', fontsize=18)
        plt.title(f"top {args.f} features",fontsize=18)
        plt.stem(xlist[::-1], ylist[::-1], orientation='horizontal', linefmt='--', markerfmt='D')
        plt.savefig( f'./Results/{os.path.splitext(os.path.basename(csvfile))[0]}.stem.png',
            dpi=300)

    feature_importance_stem_()

    logger.info("---mrmd 3.0 end---")


